# Fine-tuning configuration for Gemma-2B

model:
  name: "google/gemma-2b"
  use_lora: true
  lora_config:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 2e-5
  warmup_steps: 100
  max_length: 512
  gradient_checkpointing: true
  fp16: true
  
  # For small dataset, reduce epochs to avoid overfitting
  small_dataset_config:
    num_epochs: 2
    learning_rate: 5e-6
    
evaluation:
  eval_steps: 50
  save_strategy: "steps"
  metric_for_best_model: "loss"
  greater_is_better: false
  load_best_model_at_end: true

checkpoints:
  save_percentages: [0.25, 0.5, 0.75, 1.0]
  output_dir: "circuit_checkpoints"

logging:
  logging_steps: 10
  report_to: ["tensorboard"]
  
# Resource estimates
resources:
  min_gpu_memory: "16GB"
  recommended_gpu_memory: "24GB"
  estimated_training_time:
    full_dataset: "30-60 minutes"
    test_run: "5-10 minutes"